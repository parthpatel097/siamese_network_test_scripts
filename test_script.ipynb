{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 3507.241455078125\n",
      "3999 1627.653564453125\n",
      "5999 794.85791015625\n",
      "7999 403.08258056640625\n",
      "9999 198.22640991210938\n",
      "11999 104.01927185058594\n",
      "13999 55.05368423461914\n",
      "15999 31.630266189575195\n",
      "17999 20.212478637695312\n",
      "19999 14.45259952545166\n",
      "21999 11.642106056213379\n",
      "23999 10.217216491699219\n",
      "25999 9.507014274597168\n",
      "27999 9.179034233093262\n",
      "29999 8.996671676635742\n",
      "Result: y = -0.01326314453035593 + 0.8596036434173584 x + 0.001880655880086124 x^2 + -0.09392840415239334 x^3 + 7.896095485193655e-05 x^4 ? + 7.896095485193655e-05 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate five parameters and assign them as members.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 4, 5\n",
    "        and reuse the e parameter to compute the contribution of these orders.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same parameter many\n",
    "        times when defining a computational graph.\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet()\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class testModule(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(testModule, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 10, bias=True)\n",
    "        self.fc2 = nn.Linear(10, 10, bias=False)\n",
    "\n",
    "        # Remove the weights as we override them in the forward\n",
    "        # so that they don't show up when calling .parameters()\n",
    "        del self.fc1.weight\n",
    "        del self.fc2.weight\n",
    "\n",
    "        self.fc2_base_weights = nn.Parameter(torch.randn(10, 10))\n",
    "        self.shared_weights = nn.Parameter(torch.randn(10, 5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Update the weights\n",
    "        index = [1, 3, 5, 7, 9]\n",
    "        self.fc1.weight = self.shared_weights\n",
    "        self.fc2.weight = self.fc2_base_weights.clone()\n",
    "        self.fc2.weight[:, index] = self.shared_weights\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    # def _weight(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sharing weights between the units of the same layer and between layers \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.rand(3))  # w1, w2, w3\n",
    "    def forward(self, x):\n",
    "        x = torch.sum(x * self.w)\n",
    "        x = torch.sum(x * self.w)\n",
    "        return x\n",
    "# Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7591dd66645d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfirst_layer_node_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# n by n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfirst_layer_node_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# n by n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfirst_layer_node_3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# n by n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w1' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "b= 3\n",
    "n=1000\n",
    "input=torch.rand(b,n,n)\n",
    "\n",
    "first_layer_node_1=nn.ReLU(w1*w2*w3*input[0])# n by n\n",
    "first_layer_node_2=nn.ReLU(w1*w2*w3*input[1])# n by n\n",
    "first_layer_node_3=nn.ReLU(w1*w2*w3*input[2])# n by n\n",
    "\n",
    "\n",
    "output_layer=nn.ReLU(w1*first_layer_node_1+w2*first_layer_node_2+w3*first_layer_node_3)# n by n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "DIM = 128\n",
    "SEQ_LEN = 12\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(DIM, DIM, 5, padding=2),#nn.Linear(DIM, DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(DIM, DIM, 5, padding=2),#nn.Linear(DIM, DIM),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.res_block(input)\n",
    "        return input + (0.3*output)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, charmap1, charmap2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv0_a = nn.Conv1d(len(charmap1), DIM, 1)\n",
    "        self.conv0_b = nn.Conv1d(len(charmap2), DIM, 1)\n",
    "        self.block1_a = ResBlock()\n",
    "        self.block1_b = ResBlock()\n",
    "        self.base2 = ResBlock()\n",
    "        self.base3 = ResBlock()\n",
    "        self.base4 = ResBlock()\n",
    "        self.base5 = ResBlock()\n",
    "        self.linear = nn.Linear(SEQ_LEN*DIM, 1)\n",
    "\n",
    "    def forward(self, x_a, x_b):\n",
    "        output_a = self.block1_a(self.conv0_a(x_a.transpose(1, 2))) \n",
    "        output_b = self.block1_b(self.conv0_b(x_b.transpose(1, 2)))\n",
    "        #output = torch.cat((output_a, output_b), 0)\n",
    "        output_a = self.base2(output_a)\n",
    "        output_b = self.base2(output_b)\n",
    "        output_a = self.base3(output_a)\n",
    "        output_b = self.base3(output_b)\n",
    "        output_a = self.base4(output_a)\n",
    "        output_b = self.base4(output_b)\n",
    "        output_a = self.base5(output_a)\n",
    "        output_b = self.base5(output_b)\n",
    "        output_a = output_a.view(-1, SEQ_LEN*DIM)\n",
    "        output_b = output_b.view(-1, SEQ_LEN*DIM)\n",
    "        output_a = self.linear(output_a)\n",
    "        output_b = self.linear(output_b)\n",
    "        return output_a, output_b\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, charmap1, charmap2):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(128, DIM*SEQ_LEN)\n",
    "        self.base1 = ResBlock()\n",
    "        self.base2 = ResBlock()\n",
    "        self.base3 = ResBlock()\n",
    "        self.base4 = ResBlock()\n",
    "        self.block5_a = ResBlock()\n",
    "        self.block5_b = ResBlock()        \n",
    "        self.conv1_a = nn.Conv1d(DIM, len(charmap1), 1)\n",
    "        self.conv1_b = nn.Conv1d(DIM, len(charmap2), 1)\n",
    "        self.softmax_a = nn.Softmax()\n",
    "        self.softmax_b = nn.Softmax()\n",
    "\n",
    "    def forward(self, noise):\n",
    "        output_a = self.fc1(noise)\n",
    "        output_b = self.fc1(noise)\n",
    "        output_a = output_a.view(-1, DIM, SEQ_LEN) # (BATCH_SIZE, DIM, SEQ_LEN)\n",
    "        output_b = output_b.view(-1, DIM, SEQ_LEN)\n",
    "        output_a = self.base1(output_a)\n",
    "        output_b = self.base1(output_b)\n",
    "        output_a = self.base2(output_a)\n",
    "        output_b = self.base2(output_b)\n",
    "        output_a = self.base3(output_a)\n",
    "        output_b = self.base3(output_b)\n",
    "        output_a = self.base4(output_a)\n",
    "        output_b = self.base4(output_b)\n",
    "        output_a = self.conv1_a(self.block5_a(output_a)).transpose(1,2)\n",
    "        output_b = self.conv1_b(self.block5_b(output_b)).transpose(1,2)\n",
    "        shape_a = output_a.size()\n",
    "        shape_b = output_b.size()\n",
    "        output_a = self.softmax_a(output_a.contiguous().view(BATCH_SIZE*SEQ_LEN, -1))\n",
    "        output_b = self.softmax_b(output_b.contiguous().view(BATCH_SIZE*SEQ_LEN, -1))\n",
    "        return output_a.view(shape_a), output_b.view(shape_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "\n",
    "for param in model.parameters():\n",
    "    weights.append(param.clone())\n",
    "\n",
    "criterion = nn.BCELoss() # criterion and optimizer setup\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "foo = torch.randn(3, 10) # fake input\n",
    "target = torch.randn(3, 5) # fake target\n",
    "\n",
    "result = model(foo) # predictions and comparison and backprop\n",
    "loss = criterion(result, target)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "weights_after_backprop = [] # weights after backprop\n",
    "for param in model.parameters():\n",
    "    weights_after_backprop.append(param.clone()) # only layer1's weight should update, layer2 is not used\n",
    "\n",
    "for i in zip(weights, weights_after_backprop):\n",
    "    print(torch.equal(i[0], i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-6cd296ef3051>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6cd296ef3051>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    encoder = nn.Linear(in, out)\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "# Real off-the-shelf tied linear module\n",
    "class TiedLinear(nn.Module):\n",
    "    def __init__(self, tied_to: nn.Linear, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.tied_to = tied_to\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(tied_to.in_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # copied from nn.Linear\n",
    "    def reset_parameters(self):\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.tied_to.weight.t())\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.linear(input, self.tied_to.weight.t(), self.bias)\n",
    "\n",
    "    # To keep module properties intuitive\n",
    "    @property\n",
    "    def weight(self) -> torch.Tensor:\n",
    "        return self.tied_to.weight.t()\n",
    "\n",
    "# Shared weights, different biases\n",
    "encoder = nn.Linear(in, out)\n",
    "decoder = TiedLinear(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "feature_ex = nn.Sequential(OrderedDict([('conv1', nn.Conv2d(1, 6, 5)),\n",
    "                                        ('relu1', nn.ReLU()),\n",
    "                                        ('maxpool1', nn.MaxPool2d((2, 2))),\n",
    "                                        ('conv2', nn.Conv2d(6, 16, 5)),\n",
    "                                        ('relu2', nn.ReLU()),\n",
    "                                        ('maxpool2', nn.MaxPool2d(2))\n",
    "                                        ]))\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = feature_ex(x)   # [1]\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)     # [2]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:       # Get the products\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "model1 = Net()\n",
    "model2 = Net()\n",
    "\n",
    "img = torch.randn(10, 1, 32, 32)\n",
    "out1 = model1.forward(img)\n",
    "out2 = model2.forward(img)\n",
    "\n",
    "# [1]\n",
    "# print(np.allclose(out1.detach().numpy(), out2.detach().numpy()))\n",
    "# output: True\n",
    "\n",
    "# [2]\n",
    "print(np.allclose(out1.detach().numpy(), out2.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert images files into folder as per image name \n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "MAX_FILES_PER_DIR = 100    \n",
    "\n",
    "pngDirectory = Path('/home/parth/Documents/test_imagesimilarity/training_copy/')\n",
    "pngFiles = pngDirectory.glob('*.png')   #find all png files in directory\n",
    "for pngFile in pngFiles:\n",
    "    fileNumber = pngFile.name.split('.')[0]         #get number from filename\n",
    "  \n",
    "    currentFolder = pngDirectory / \"{}\".format(fileNumber)\n",
    "    if os.path.isdir(fileNumber) :\n",
    "       pass\n",
    "    else:\n",
    "        currentFolder.mkdir()\n",
    "        shutil.copy(pngFile,currentFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
